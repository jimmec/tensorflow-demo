{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Demo\n",
    "playing with the [Deep ConvNet for MNIST digit classification](https://www.tensorflow.org/versions/master/tutorials/mnist/pros/index.html#build-a-multilayer-convolutional-network) tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow.examples.tutorials.mnist.input_data as input_data\n",
    "mnist = input_data.read_data_sets('MNIST-data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple softmax regression demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# set input/output placeholders\n",
    "x = tf.placeholder(\"float\", shape=[None, 784])\n",
    "y_ = tf.placeholder(\"float\", shape=[None, 10])\n",
    "# define the model parameters as tf.Variable's\n",
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# initialize them in our InteractiveSession\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "y = tf.nn.softmax(tf.matmul(x,W) + b)\n",
    "\n",
    "# Loss function = the discrete cross entropy between the predicted distributions and the labels 1-hot vectors\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "\n",
    "# use steepest descent to minmize loss function, \n",
    "# train_step is an op that will compute gradient, compute update step, apply update\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "\n",
    "# So to find min, just repeated run the train_step\n",
    "for i in xrange(1000):\n",
    "    if i % 100 == 0:\n",
    "        print i\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    # note, using feed_dict can replace any tensor in graph, not just tf.placeholders\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1]})\n",
    "    \n",
    "# evaluate\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accu = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n",
    "\n",
    "print(accu.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "A = np.random.random([3,3])\n",
    "print(A)\n",
    "b = np.array([[1],[2],[3]]).astype(float)\n",
    "sess = tf.Session()\n",
    "print(sess.run(tf.constant(A)+tf.constant(b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep ConvNet demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of negative examples: 2017363\n",
      "num of positive examples: 2745401"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "import itertools\n",
    "import cPickle\n",
    "from collections import Counter\n",
    "import re\n",
    "\"\"\"\n",
    "Code adapted from https://github.com/dennybritz/cnn-text-classification-tf\n",
    "\"\"\"\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for input.\n",
    "    Originally taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = tokenize(string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "def tokenize(string):\n",
    "    \"\"\"\n",
    "    Tokenize English strings\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string) \n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string) \n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string) \n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
    "    string = re.sub(r\",\", \" , \", string) \n",
    "    string = re.sub(r\"!\", \" ! \", string) \n",
    "    string = re.sub(r\"\\(\", \" ( \", string) \n",
    "    string = re.sub(r\"\\)\", \" ) \", string) \n",
    "    string = re.sub(r\"\\?\", \" ? \", string) \n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string) \n",
    "    return string\n",
    "    \n",
    "def normalize_sentence_length(sentences, padding_word='<PAD/>', max_length_words = None):\n",
    "    \"\"\"\n",
    "    Normalizes lengths to max_length. max_length = None, normalizes to max sentence length.\n",
    "    Pads shorter sentences using the padding_word. \n",
    "    Cuts longer sentences at max_length number of words.\n",
    "    \n",
    "    sentences - list of sentence where each sentence is a list of words. eg. [['foo','bar'], ['fo2','bar2']]\n",
    "    \"\"\"\n",
    "    max_length_words = max_length_words if max_length_words is not None else max(len(x) for x in sentences)\n",
    "    norm_sentences = []\n",
    "    for i in xrange(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        num_padding = max_length_words - len(sentence)\n",
    "        padded_sentence = sentence + [padding_word] * num_padding\n",
    "        chopped_sentence = padded_sentence[:max_length_words]\n",
    "        norm_sentences.append(chopped_sentence)\n",
    "    return norm_sentences\n",
    "\n",
    "def raw_text_to_sentence(text):\n",
    "    return clean_str(text).split(\" \")\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary mapping from word to index based on the sentences.\n",
    "    Returns vocabulary mapping and inverse vocabulary mapping.\n",
    "    \"\"\"\n",
    "    print('building vocab...')\n",
    "    # Build vocabulary\n",
    "    word_counts = Counter(itertools.chain(*sentences))\n",
    "    # Mapping from index to word\n",
    "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "    # Mapping from word to index\n",
    "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "    return [vocabulary, vocabulary_inv]\n",
    "\n",
    "def build_input_data(sentences, labels, vocabulary):\n",
    "    \"\"\"\n",
    "    Maps sentences to vectors based on vocabulary.\n",
    "    Maps binary labels to 1 hot vectors, 0 -> [1,0], 1 -> [0,1]\n",
    "    returns np.arrays\n",
    "    \"\"\"\n",
    "    x = np.array([[vocabulary[word] for word in sentence] for sentence in sentences])\n",
    "    y = np.array([[0,1] if label else [1,0] for label in labels])\n",
    "\n",
    "def read_pos_and_neg_data(header=True):\n",
    "    with open('./data/raw_input_neg.tsv', 'rb') as f:\n",
    "        f.readline() if header else None\n",
    "        lines = f.readlines()\n",
    "        negatives = [(0, line.split('\\t')[3]) for line in lines]\n",
    "        print('num of negative examples: {}'.format(len(negatives)))\n",
    "    with open('./data/raw_input_pos.tsv', 'rb') as f:\n",
    "        f.readline() if header else None\n",
    "        lines = f.readlines()\n",
    "        positives = [(1, line) for line in lines]\n",
    "        print('num of positive examples: {}'.format(len(positives)))\n",
    "    return positives + negatives\n",
    "\n",
    "def write_dev_train_sets(label_texts, path_template, p=0.1):\n",
    "    \"\"\"\n",
    "    Outputs a dev data set using apprx p percent of the list of label and text tuples label_texts\n",
    "    Rest goes to the training set\n",
    "    \"\"\"\n",
    "    print('saving dev and training data sets...')\n",
    "    # sample p% for dev use \n",
    "    dev_mask = np.random.rand(len(label_texts)) <= p\n",
    "    train_mask = np.array([not d for d in dev_mask])\n",
    "    full = np.array(label_texts)\n",
    "    dev = full[dev_mask]\n",
    "    train = full[train_mask]\n",
    "    \n",
    "    with open(path_template.format('dev'), 'w') as df:\n",
    "        for pair in dev:\n",
    "            df.write(str.format('{}\\t{}\\n', pair[0], pair[1]))\n",
    "    with open(path_template.format('train'), 'w') as tf:\n",
    "        for pair in train:\n",
    "            tf.write(str.format('{}\\t{}\\n', pair[0], pair[1]))\n",
    "\n",
    "def preprocess_raw_inputs_and_save():\n",
    "    labels, texts = zip(*read_pos_and_neg_data())\n",
    "    sentences = map(raw_text_to_sentence, texts)\n",
    "    normalized_sentences = normalize_sentence_length(sentences)\n",
    "    vocab, inv_vocab = build_vocab(normalized_sentences)\n",
    "    \n",
    "    ts = datetime.datetime.now().strftime('%Y-%m-%d.%H%M%S')\n",
    "    path_template = './data/{{}}_set.{}'.format(ts)\n",
    "    with open('./data/vocab.pickle.{}'.format(ts), 'w') as vocab_file:\n",
    "        cPickle.dump({'vocabulary': vocab, 'inv_vocabulary': inv_vocab}, vocab_file)\n",
    "    #write_dev_train_sets(zip(labels, texts), path_template, p=0.1)\n",
    "    \n",
    "preprocess_raw_inputs_and_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
