{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Demo\n",
    "playing with the [Deep ConvNet for MNIST digit classification](https://www.tensorflow.org/versions/master/tutorials/mnist/pros/index.html#build-a-multilayer-convolutional-network) tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow.examples.tutorials.mnist.input_data as input_data\n",
    "mnist = input_data.read_data_sets('MNIST-data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple softmax regression demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# set input/output placeholders\n",
    "x = tf.placeholder(\"float\", shape=[None, 784])\n",
    "y_ = tf.placeholder(\"float\", shape=[None, 10])\n",
    "# define the model parameters as tf.Variable's\n",
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# initialize them in our InteractiveSession\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "y = tf.nn.softmax(tf.matmul(x,W) + b)\n",
    "\n",
    "# Loss function = the discrete cross entropy between the predicted distributions and the labels 1-hot vectors\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "\n",
    "# use steepest descent to minmize loss function, \n",
    "# train_step is an op that will compute gradient, compute update step, apply update\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "\n",
    "# So to find min, just repeated run the train_step\n",
    "for i in range(1000):\n",
    "    if i % 100 == 0:\n",
    "        print i\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    # note, using feed_dict can replace any tensor in graph, not just tf.placeholders\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1]})\n",
    "    \n",
    "# evaluate\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accu = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n",
    "\n",
    "print(accu.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "A = np.random.random([3,3])\n",
    "print(A)\n",
    "b = np.array([[1],[2],[3]]).astype(float)\n",
    "sess = tf.Session()\n",
    "print(sess.run(tf.constant(A)+tf.constant(b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep ConvNet demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['adsfadsf', 'adsfa', 'adsfa', 'dfd'],\n",
       " ['asdf', 'asd', 'f', 'asdfsfd', 'sdfad']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "import itertools\n",
    "import cPickle\n",
    "from collections import Counter\n",
    "import re\n",
    "\"\"\"\n",
    "Code adapted from https://github.com/dennybritz/cnn-text-classification-tf\n",
    "\"\"\"\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for input.\n",
    "    Originally taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = tokenize(string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "def tokenize(string):\n",
    "    \"\"\"\n",
    "    Tokenize English strings\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string) \n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string) \n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string) \n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
    "    string = re.sub(r\",\", \" , \", string) \n",
    "    string = re.sub(r\"!\", \" ! \", string) \n",
    "    string = re.sub(r\"\\(\", \" ( \", string) \n",
    "    string = re.sub(r\"\\)\", \" ) \", string) \n",
    "    string = re.sub(r\"\\?\", \" ? \", string) \n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string) \n",
    "    return string\n",
    "    \n",
    "def normalize_sentence_length(sentences, padding_word='<PAD/>', max_length_words = None):\n",
    "    \"\"\"\n",
    "    Normalizes lengths to max_length. max_length = None, normalizes to max sentence length.\n",
    "    Pads shorter sentences using the padding_word. \n",
    "    Cuts longer sentences at max_length number of words.\n",
    "    \n",
    "    sentences - list of sentence where each sentence is a list of words. eg. [['foo','bar'], ['fo2','bar2']]\n",
    "    \"\"\"\n",
    "    max_length_words = max_length_words if max_length_words is not None else max(len(x) for x in sentences)\n",
    "    norm_sentences = []\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        num_padding = max_length_words - len(sentence)\n",
    "        padded_sentence = sentence + [padding_word] * num_padding\n",
    "        chopped_sentence = padded_sentence[:max_length_words]\n",
    "        norm_sentences.append(chopped_sentence)\n",
    "    return norm_sentences\n",
    "\n",
    "def raw_text_to_sentence(text):\n",
    "    return clean_str(text).split(\" \")\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary mapping from word to index based on the sentences.\n",
    "    Returns vocabulary mapping and inverse vocabulary mapping.\n",
    "    \"\"\"\n",
    "    print('building vocab...')\n",
    "    # Build vocabulary\n",
    "    word_counts = Counter(itertools.chain(*sentences))\n",
    "    # Mapping from index to word\n",
    "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "    # Mapping from word to index\n",
    "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "    return [vocabulary, vocabulary_inv]\n",
    "\n",
    "def build_input_data(sentences, labels, vocabulary):\n",
    "    \"\"\"\n",
    "    Maps sentences to vectors based on vocabulary.\n",
    "    Maps binary labels to 1 hot vectors, 0 -> [1,0], 1 -> [0,1]\n",
    "    returns np.arrays\n",
    "    \"\"\"\n",
    "    x = np.array([[vocabulary[word] for word in sentence] for sentence in sentences])\n",
    "    y = np.array([[0,1] if label else [1,0] for label in labels])\n",
    "\n",
    "def read_pos_and_neg_data(header=True):\n",
    "    with open('./data/raw_input_neg.tsv', 'rt') as f:\n",
    "        f.readline() if header else None\n",
    "        lines = f.readlines()\n",
    "        negatives = [(0, line.split('\\t')[3]) for line in lines]\n",
    "        print('num of negative examples: {}'.format(len(negatives)))\n",
    "    with open('./data/raw_input_pos.tsv', 'rt') as f:\n",
    "        f.readline() if header else None\n",
    "        lines = f.readlines()\n",
    "        positives = [(1, line) for line in lines]\n",
    "        print('num of positive examples: {}'.format(len(positives)))\n",
    "    return positives + negatives\n",
    "\n",
    "def write_dev_train_sets(label_texts, path_template, p=0.1):\n",
    "    \"\"\"\n",
    "    Outputs a dev data set using apprx p percent of the list of label and text tuples label_texts\n",
    "    Rest goes to the training set\n",
    "    \"\"\"\n",
    "    print('saving dev and training data sets...')\n",
    "    # sample p% for dev use \n",
    "    dev_mask = np.random.rand(len(label_texts)) <= p\n",
    "    train_mask = np.array([not d for d in dev_mask])\n",
    "    full = np.array(label_texts)\n",
    "    dev = full[dev_mask]\n",
    "    train = full[train_mask]\n",
    "    \n",
    "    with open(path_template.format('dev'), 'wt') as df:\n",
    "        for pair in dev:\n",
    "            df.write(str.format('{}\\t{}\\n', pair[0], pair[1]))\n",
    "    with open(path_template.format('train'), 'wt') as tf:\n",
    "        for pair in train:\n",
    "            tf.write(str.format('{}\\t{}\\n', pair[0], pair[1]))\n",
    "\n",
    "def preprocess_raw_inputs_and_save():\n",
    "    labels, texts = zip(*read_pos_and_neg_data())\n",
    "    sentences = map(raw_text_to_sentence, texts)\n",
    "    normalized_sentences = normalize_sentence_length(sentences)\n",
    "    vocab, inv_vocab = build_vocab(normalized_sentences)\n",
    "    \n",
    "    ts = datetime.datetime.now().strftime('%Y-%m-%d.%H%M%S')\n",
    "    path_template = './data/{{}}_set.{}'.format(ts)\n",
    "    with open('./data/vocab.pickle.{}'.format(ts), 'wt') as vocab_file:\n",
    "        pickle.dump({'vocabulary': vocab, 'inv_vocabulary': inv_vocab}, vocab_file)\n",
    "    write_dev_train_sets(zip(labels, [\" \".join(sent) for sent in normalized_sentences]), path_template, p=0.1)\n",
    "    \n",
    "def load_input_data(example_path, vocab_path):\n",
    "    with open(example_path, 'rt') as f:\n",
    "        lines = f.readlines()\n",
    "        labels = [line.split('\\t')[0] for line in lines]\n",
    "        texts = [line.split('\\t')[1] for line in lines]\n",
    "        sentences = map(raw_text_to_sentence, texts)\n",
    "    with open(vocab_path, 'rb') as v:\n",
    "        vocab_map = pickle.load(v)\n",
    "    return build_input_data(sentences, labels, vocab_map['vocabulary'])\n",
    "\n",
    "preprocess_raw_inputs_and_save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN class to define computation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "      self, sequence_length, num_classes, vocab_size,\n",
    "      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            W = tf.Variable(\n",
    "                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "                name=\"W\")\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(3, pooled_outputs)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.Variable(tf.truncated_normal([num_filters_total, num_classes], stddev=0.1), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        # CalculateMean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(self.scores, self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
